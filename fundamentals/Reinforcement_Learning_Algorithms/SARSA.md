# SARSA (State–Action–Reward–State–Action)

SARSA GridWorld: 5x5 grid with a blue start at (0,0), an orange goal at (4,4), black blocked cells at (1,2) and (3,2), and a green best path from start to goal.

SARSA is a model-free reinforcement learning algorithm that learns an optimal policy through direct environmental interaction. Unlike Q-learning, which updates its Q-values based on the maximum Q-value of the next state, SARSA updates its Q-values based on the Q-value of the next state AND the actual action taken in that state.

This key difference makes SARSA an on-policy algorithm, meaning it learns the value of the policy it is currently following. Q-learning is off-policy because it learns the value of the optimal policy independent of the current policy.

--------------------------------------------------

## SARSA Update Rule

Q(s, a) ← Q(s, a) + α * (r + γ * Q(s', a') − Q(s, a))

Where:
- s  = current state  
- a  = current action  
- r  = reward received  
- s' = next state  
- a' = next action taken  
- α  = learning rate  
- γ  = discount factor  

The term Q(s', a') reflects the expected future reward for the next state–action pair, based on the **current policy** instead of the best possible action.

This conservative, on-policy approach makes SARSA more suitable for environments where safety and stability are important, while Q-learning may find a more optimal policy faster but with riskier exploration.

--------------------------------------------------

## Intuition Example

Imagine a robot learning to navigate a room with obstacles.

SARSA guides the robot to learn a safer path by considering:
- The reward of the current action
- The consequences of the next action it actually chooses

Even if a risky move looks promising, SARSA may avoid it because it has learned that the next step under its current policy could be harmful.

This makes SARSA more cautious in practice.

--------------------------------------------------

## The SARSA Algorithm (Step-by-Step)

1. Initialization  
   Initialize the Q-table with arbitrary values (usually 0) for each state-action pair.

2. Choose an Action  
   In the current state s, choose an action a using an exploration-exploitation strategy (e.g., epsilon-greedy).

3. Take Action and Observe  
   Execute action a and observe:
   - the next state s'
   - the reward r

4. Choose Next Action  
   In state s', choose the next action a' using the current policy (e.g., epsilon-greedy).

5. Update Q-Value  
   Update Q(s, a) using the SARSA update equation.

6. Update State and Action  
   Set:
   - s = s'
   - a = a'

7. Repeat  
   Continue steps 3–6 until the Q-values converge or a maximum number of iterations is reached.

This iterative process allows the agent to learn and refine its policy over time.

--------------------------------------------------

## On-Policy vs Off-Policy Learning

In reinforcement learning, algorithms can be grouped into two categories:

### On-policy learning
- Learns the value of the **current policy**
- Updates are based on the actions actually taken
- Includes exploration behavior in learning
- Example: SARSA

### Off-policy learning
- Learns the optimal policy independent of current behavior
- Can learn from data generated by a different policy
- More aggressive and exploratory
- Example: Q-learning

Flow idea:
- If you choose SARSA → follow current policy  
- If you choose Q-learning → choose greedy action (max Q)  
- Both update Q-values, but with different philosophies

--------------------------------------------------

## Why SARSA is Different

SARSA uses:
Q(s', a') → the actual next action chosen under the current policy

Q-learning uses:
max Q(s', a') → the best possible action, regardless of current policy

This has real effects:

- Sarah learns the value of its *actual* behavior
- It is more stable and safer
- It is less likely to take dangerous exploratory paths
- It may converge more slowly than Q-learning
- The learned policy reflects the exploration strategy

It essentially learns “on the job” while following its own decisions.

--------------------------------------------------

## Exploration-Exploitation in SARSA

SARSA still faces the exploration–exploitation dilemma:

- Explore → Try new actions
- Exploit → Use known optimal actions

Because SARSA is on-policy, exploration directly affects learning more than in Q-learning.

--------------------------------------------------

## Epsilon-Greedy Strategy in SARSA

With probability ε:
- Choose a random action (explore)

With probability 1 − ε:
- Choose action with highest Q-value (exploit)

Because SARSA updates with the *actual* chosen next action, this exploratory choice is baked into learning and results in more cautious overall behavior.

--------------------------------------------------

## Softmax Strategy in SARSA

Instead of choosing completely random actions, **Softmax** assigns probabilities to actions based on their Q-values:

- Higher Q-values → higher probability
- Lower Q-values → lower probability

This allows for smoother, more balanced exploration and can lead to more stable, intelligent learning behavior.

--------------------------------------------------

## Convergence & Parameter Tuning

SARSA converges when Q-values stop changing significantly.

Important parameters:

1. Learning Rate (α)
   - High α → Faster learning, but more unstable
   - Low α → Slower but more stable learning

2. Discount Factor (γ)
   - High γ (close to 1) → Future rewards matter more
   - Low γ → Focus on immediate rewards

Convergence depends on:
- Proper α and γ
- A good exploration strategy
- Adequate coverage of state-action pairs

If all state-action pairs are visited enough times and learning rate is controlled, SARSA can converge to an optimal policy under the given strategy.

--------------------------------------------------

## Data Assumptions

Like Q-learning, SARSA assumes:

1. Markov Property  
   The next state depends only on the current state and action.

2. Stationary Environment  
   The environment’s dynamics and reward structure do not change over time.

--------------------------------------------------

SARSA is well-suited for environments where **safety, stability, and controlled behavior** are more important than aggressive optimization. Its on-policy nature allows agents to learn reliable, predictable strategies while avoiding potentially dangerous actions.
